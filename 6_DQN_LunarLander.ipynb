{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCLrRFHSKl_5"
   },
   "source": [
    "# Deep Q-Network with Lunar Lander\n",
    "\n",
    "This notebook shows an implementation of a DQN on the LunarLander environment.\n",
    "Details on the environment can be found [here](https://gymnasium.farama.org/environments/box2d/lunar_lander/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2RNqaAGiLU1"
   },
   "source": [
    "## 1. Setup\n",
    "\n",
    "We first need to install some dependencies for using the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96dExX1TKm2m",
    "outputId": "4a3a6d38-7cae-4c85-dc5f-c0182c2e8275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (1.24.3)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (6.0.0)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame==2.1.3 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (2.1.3)\n",
      "Requirement already satisfied: swig==4.* in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (4.1.1.post0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install 'gymnasium[box2d]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CZXskDwXKl_-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from time import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVO0INWR1DYS",
    "outputId": "68e453b9-79a7-4921-ac7c-186de15ec6c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fad1db18e10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lrq9VwzO1Zx4",
    "outputId": "ae7034e1-9628-4794-bba6-f7dac7ad5de4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKg3BvSnivPE"
   },
   "source": [
    "## 2. Define the neural network, the replay buffer and the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9pG_Ii7jToR"
   },
   "source": [
    "First, we define the neural network that predicts the Q-values for all actions, given a state as input.\n",
    "This is a fully-connected neural net with two hidden layers using Relu activations.\n",
    "The last layer does not have any activation and outputs a Q-value for every action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JFxqeLkf1eHY"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0xHaPTIj1pD"
   },
   "source": [
    "Next, we define a replay buffer that saves previous transitions and provides a `sample` function to randomly extract a batch of experiences from the buffer.\n",
    "\n",
    "Note that experiences are internally saved as `numpy`-arrays. They are converted back to PyTorch tensors before being returned by the `sample`-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gQw6QVAC1hQf"
   },
   "outputs": [],
   "source": [
    "class StateTransition:\n",
    "    def __init__(self, state, action, reward, next_state, done):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.done = 1 if done else 0 # Convert done flag from boolean to int\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "       \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        state_transition = StateTransition(state, action, reward, next_state, done)\n",
    "        self.memory.append(state_transition)\n",
    "                \n",
    "    def sample(self):\n",
    "        state_transitions = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        states = np.vstack([s_t.state for s_t in state_transitions])\n",
    "        states_tensor = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "        actions = np.vstack([s_t.action for s_t in state_transitions])\n",
    "        actions_tensor = torch.from_numpy(actions).long().to(device)\n",
    "\n",
    "        rewards = np.vstack([s_t.reward for s_t in state_transitions])\n",
    "        rewards_tensor = torch.from_numpy(rewards).float().to(device)\n",
    "\n",
    "        next_states = np.vstack([s_t.next_state for s_t in state_transitions])\n",
    "        next_states_tensor = torch.from_numpy(next_states).float().to(device)\n",
    "        \n",
    "        dones = np.vstack([s_t.done for s_t in state_transitions])\n",
    "        dones_tensor = torch.from_numpy(dones).float().to(device)\n",
    "        \n",
    "        return (states_tensor, actions_tensor, rewards_tensor, next_states_tensor, dones_tensor)\n",
    "        \n",
    "    def is_filled(self):\n",
    "        return len(self.memory) >= BATCH_SIZE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IYjlS7Fy1jJA"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000    # Replay memory size\n",
    "BATCH_SIZE = 64         # Number of experiences to sample from memory\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TARGET_SYNC = 20        # How often the target networks is synchronized\n",
    "       \n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Initialize Q and Target Q networks\n",
    "        self.q_network = QNetwork(state_size, action_size).to(device)\n",
    "        self.target_network = QNetwork(state_size, action_size).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        \n",
    "        # Initiliase replay buffer \n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        self.timestep = 0\n",
    "    \n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.timestep += 1\n",
    "        \n",
    "        if not self.memory.is_filled(): # train only when buffer is filled\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
    "               \n",
    "        # you need to implement the following method in task 5\n",
    "        loss = self.calculate_loss(states, actions, rewards, next_states, dones) \n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Synchronize target network by copying weights\n",
    "        if self.timestep % TARGET_SYNC == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    \n",
    "    def calculate_loss(self, states, actions, rewards, next_states, dones):\n",
    "    \n",
    "        # impelement your code here\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def choose_action(self, state, epsilon):\n",
    "        rnd = random.random()\n",
    "        if rnd < epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            q_values = self.q_network(state)\n",
    "            action = np.argmax(q_values.cpu().data.numpy())\n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2P3-UIm0fh3W"
   },
   "source": [
    "### 3. Executes episodes and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NikAZhjNfsoi"
   },
   "source": [
    "We first define the necessary paramters for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lJGrZry81pu4"
   },
   "outputs": [],
   "source": [
    "TARGET_SCORE = 200            # Train until this score is reached\n",
    "MAX_EPISODE_LENGTH = 1000     # Max steps allowed in a single episode\n",
    "EPSILON_MIN = 0.01            # Minimum epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezOn9IpKf17C"
   },
   "source": [
    "Then we start executing episodes and observe the mean score per episode.\n",
    "The environment is considered as solved if this score is above 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_EC7XLJ1slY",
    "outputId": "ad110ade-36e5-4600-fcf0-faf2edc07235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 8, action size: 4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m action \u001b[38;5;241m=\u001b[39m dqn_agent\u001b[38;5;241m.\u001b[39mchoose_action(state, epsilon)\n\u001b[1;32m     23\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state        \n\u001b[1;32m     27\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward        \n",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     29\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# you need to implement the following method in task 5\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m, in \u001b[0;36mDQNAgent.calculate_loss\u001b[0;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, states, actions, rewards, next_states, dones):\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# impelement your code here\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Get state and action sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(f'State size: {state_size}, action size: {action_size}')\n",
    "dqn_agent = DQNAgent(state_size, action_size)\n",
    "start = time()\n",
    "last_time = start\n",
    "\n",
    "scores_window = deque(maxlen=100)\n",
    "mean_score = 0\n",
    "episode = 0\n",
    "\n",
    "while True:\n",
    "    episode += 1\n",
    "    score = 0\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    for t in range(MAX_EPISODE_LENGTH):\n",
    "        \n",
    "        epsilon = max(1/episode, EPSILON_MIN)\n",
    "        action = dqn_agent.choose_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        dqn_agent.train(state, action, reward, next_state, done)\n",
    "        state = next_state        \n",
    "        score += reward        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scores_window.append(score)\n",
    "    mean_score = np.mean(scores_window)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f'After {episode} episodes, average score is {mean_score:.2f}. ', end='')\n",
    "        print(f'Took {time()-last_time:.0f} seconds.')\n",
    "        last_time = time()\n",
    "    \n",
    "    if mean_score >= TARGET_SCORE:\n",
    "        print(f'Environment solved in {episode} episodes. Average score: {mean_score:.2f}')\n",
    "        break\n",
    "\n",
    "print(f'Took {time()-start:.0f} seconds (~{(time()-start)//60} minutes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement the following functions to make the code above work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the agent train something, we need to implement the `calculate_loss` function in the code above. To make this easier, we do this along the following mini tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, were are given a tiny replay buffer that contains only two transitions. Each transition has the following elements: `state`, `action`, `reward`, `next_state` and a `done` flag.\n",
    "\n",
    "\n",
    "The resulting tensors `states`, `actions`, `rewards`, `next_states` and `dones` are of the same format as the input to the function `calculate_loss`. \n",
    "\n",
    "Note: You need to execute all cells above once to use the following code snippets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 8])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "state_1 = [ 0.64,  0.38,  0.04, -0.10, -0.22, -.00,  0.00,  0.00]\n",
    "state_2 = [ 0.00,  0.35,  0.41, -0.59, -0.66, -0.23,  0.00,  0.00]\n",
    "states = torch.FloatTensor([state_1, state_2])\n",
    "print(states.shape)\n",
    "\n",
    "actions = torch.LongTensor([[2],[1]])\n",
    "print(actions.shape)\n",
    "\n",
    "rewards = torch.FloatTensor([[1.8670],[1.2630]])\n",
    "print(rewards.shape)\n",
    "\n",
    "next_state_1 = [-0.60,  0.94, -0.04, -0.13,  0.27, 0.70,  0.00,  0.00]\n",
    "next_state_2 = [-0.10,  0.04, -0.12, -0.23,  0.45, -0.20,  0.00,  0.00]\n",
    "next_states = torch.FloatTensor([next_state_1, next_state_2])\n",
    "print(next_states.shape)\n",
    "\n",
    "dones = torch.FloatTensor([[0],[1]]) # = 1 if epsiode is over, 0 else\n",
    "print(dones.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we use the approach of Deep Q-Learning.\n",
    "That means that we are given a batch of transitions as input (= 2 in our mini example here) and define the loss between the networks prediction and the Q-Learning target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first calculate the Q-Learning target. In a first step we use the `target_network` to calculate the Q-values for every state in the `next_states` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0384, 0.3251, 0.0111, 0.0878],\n",
       "        [0.0457, 0.2495, 0.0236, 0.0877]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = dqn_agent.target_network(next_states)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not want to backpropagate on these values, we detach them from the computational graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0384, 0.3251, 0.0111, 0.0878],\n",
       "        [0.0457, 0.2495, 0.0236, 0.0877]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = q_values.detach()\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using Q-Learning, we are only interested in the maximum value per line.\n",
    "Implement some code that squashes the above to a torch tensor of shape `[2, 1]` that contains for every state only the maximum Q-value.\n",
    "For instance, the tensor `[[0.0384, 0.3251, 0.0111, 0.0878], [0.0457, 0.2495, 0.0236, 0.0877]]` should be squashed to `[[0.0878], [0877]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_q_values = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to calcualte the Q-Learning targets using the tensors `rewards` and `dones` as seen in the lecture. Remember: The target consist only of the reward if the done flag is set to zero for a transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "Q_targets = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now caluclate the predicton of the network on the `states`. For this we use the `q_network` of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dqn_agent.q_network(states)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns for every state the Q-values for all actions. However, we only need the Q-values of the action that was actually taken in the transition (this is stored in `actions`).\n",
    "Next, extract the Q-value for the action actually taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_action = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values can now be used to define the loss for the current batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 3:\n",
    "Use the code from these examples to implement the `calculate_loss` function from above and train the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 4:\n",
    "Use the trained model to play and record one episode. The recorded video will be stored into the `video`-subfolder on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install moviepy\n",
    "!pip install ffmpeg --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, \"video\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "total_reward = 0.0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        action_values = dqn_agent.q_network(state)\n",
    "        action = np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "5_DQN_LunarLander.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
