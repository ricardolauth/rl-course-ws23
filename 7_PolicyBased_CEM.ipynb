{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCLrRFHSKl_5"
   },
   "source": [
    "# Lunar Lander with Cross-Entropy Method\n",
    "\n",
    "In this notebook we look at the lunar lander environment and solve it with the cross-entropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96dExX1TKm2m",
    "outputId": "59a0cc23-613d-4378-8de6-2b4d280e9fa9"
   },
   "outputs": [],
   "source": [
    "#!pip3 install 'gymnasium[box2d]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CZXskDwXKl_-"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import deque\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhZ0fzBkKmAA"
   },
   "source": [
    "# Neural Network\n",
    "\n",
    "We define a simple neural network that generates the action scores based on a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vWQr7TZgKmAB"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zFMlVViKmAE"
   },
   "source": [
    "# Generate Episodes\n",
    "\n",
    "We generate a batch of episodes and remember the traversed states, actions and rewards. To select the next action we use the output of the network. For this we first pass the scores through a softmax to get probabilites. In the second step we sampel from this distribution to get the next action to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AIiayltZKmAF"
   },
   "outputs": [],
   "source": [
    "def generate_batch(env, batch_size, t_max=5000):\n",
    "    \n",
    "    \n",
    "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
    "\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        states, actions = [], []\n",
    "        total_reward = 0\n",
    "        s, _ = env.reset(seed=0)\n",
    "        for t in range(t_max):\n",
    "            \n",
    "            s_v = torch.FloatTensor([s])\n",
    "            act_probs_v = activation(net(s_v))\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            a = np.random.choice(len(act_probs), p=act_probs)\n",
    "\n",
    "            new_s, r, done, _, _ = env.step(a)\n",
    "\n",
    "            # record sessions like you did before\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "\n",
    "            s = new_s\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "            rewards_with_idx = []    \n",
    "            for (idx, item) in enumerate(batch_rewards):\n",
    "                rewards_with_idx.append((item, idx))\n",
    "            \n",
    "            rewards_with_idx.sort(key=lambda r: r[0], reverse=True)\n",
    "            top20 = [item[1] for item in rewards_with_idx[:20]]\n",
    "\n",
    "\n",
    "            batch_states = [x for (idx, x) in enumerate(batch_states) if idx in top20]\n",
    "            batch_actions = [x for (idx, x) in enumerate(batch_actions) if idx in top20]\n",
    "            batch_rewards = [x for (idx, x) in enumerate(batch_rewards) if idx in top20]\n",
    "                \n",
    "    \n",
    "\n",
    "    return batch_states, batch_actions, batch_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvq2ZIvlKmAJ"
   },
   "source": [
    "# Training\n",
    "\n",
    "In the training step, we first use the neural network to generate a batch of episodes and then use the state-action pairs to improve the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFUzEnaDKmAJ",
    "outputId": "a5344f76-e542-4566-808e-8864fcdd4f09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.364, reward_mean=3.2, threshold=0.1\n",
      "1: loss=1.405, reward_mean=-23.3, threshold=0.1\n",
      "2: loss=1.407, reward_mean=20.6, threshold=0.1\n",
      "3: loss=1.348, reward_mean=7.8, threshold=0.1\n",
      "4: loss=1.307, reward_mean=28.7, threshold=0.1\n",
      "5: loss=1.327, reward_mean=26.6, threshold=0.1\n",
      "6: loss=1.374, reward_mean=31.0, threshold=0.1\n",
      "7: loss=1.231, reward_mean=29.8, threshold=0.1\n",
      "8: loss=1.339, reward_mean=-0.5, threshold=0.1\n",
      "9: loss=1.378, reward_mean=41.3, threshold=0.1\n",
      "10: loss=1.297, reward_mean=25.0, threshold=0.1\n",
      "11: loss=1.361, reward_mean=39.3, threshold=0.1\n",
      "12: loss=1.357, reward_mean=33.5, threshold=0.1\n",
      "13: loss=1.337, reward_mean=37.6, threshold=0.1\n",
      "14: loss=1.328, reward_mean=49.0, threshold=0.1\n",
      "15: loss=1.282, reward_mean=56.8, threshold=0.1\n",
      "16: loss=1.351, reward_mean=53.9, threshold=0.1\n",
      "17: loss=1.282, reward_mean=42.4, threshold=0.1\n",
      "18: loss=1.327, reward_mean=52.7, threshold=0.1\n",
      "19: loss=1.341, reward_mean=59.9, threshold=0.1\n",
      "20: loss=1.405, reward_mean=54.6, threshold=0.1\n",
      "21: loss=1.335, reward_mean=60.9, threshold=0.1\n",
      "22: loss=1.289, reward_mean=49.9, threshold=0.1\n",
      "23: loss=1.277, reward_mean=60.3, threshold=0.1\n",
      "24: loss=1.311, reward_mean=53.7, threshold=0.1\n",
      "25: loss=1.228, reward_mean=57.6, threshold=0.1\n",
      "26: loss=1.261, reward_mean=62.7, threshold=0.1\n",
      "27: loss=1.365, reward_mean=60.1, threshold=0.1\n",
      "28: loss=1.263, reward_mean=69.3, threshold=0.1\n",
      "29: loss=1.260, reward_mean=63.7, threshold=0.1\n",
      "30: loss=1.289, reward_mean=69.3, threshold=0.1\n",
      "31: loss=1.047, reward_mean=67.2, threshold=0.1\n",
      "32: loss=1.271, reward_mean=66.6, threshold=0.1\n",
      "33: loss=1.180, reward_mean=68.3, threshold=0.1\n",
      "34: loss=0.950, reward_mean=52.1, threshold=0.1\n",
      "35: loss=1.307, reward_mean=64.1, threshold=0.1\n",
      "36: loss=1.185, reward_mean=61.1, threshold=0.1\n",
      "37: loss=1.276, reward_mean=65.8, threshold=0.1\n",
      "38: loss=0.770, reward_mean=62.9, threshold=0.1\n",
      "39: loss=1.173, reward_mean=60.0, threshold=0.1\n",
      "40: loss=1.174, reward_mean=41.3, threshold=0.1\n",
      "41: loss=1.119, reward_mean=66.1, threshold=0.1\n",
      "42: loss=1.149, reward_mean=65.3, threshold=0.1\n",
      "43: loss=1.142, reward_mean=58.4, threshold=0.1\n",
      "44: loss=1.233, reward_mean=70.0, threshold=0.1\n",
      "45: loss=1.090, reward_mean=76.4, threshold=0.1\n",
      "46: loss=1.126, reward_mean=69.9, threshold=0.1\n",
      "47: loss=0.912, reward_mean=73.3, threshold=0.1\n",
      "48: loss=1.255, reward_mean=64.8, threshold=0.1\n",
      "49: loss=1.321, reward_mean=151.1, threshold=0.1\n",
      "Environment has been successfullly completed!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "session_size = 100\n",
    "hidden_size = 200\n",
    "completion_score = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(n_states, hidden_size, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(session_size):\n",
    "    \n",
    "    # generate new episodes\n",
    "    states, actions, rewards = generate_batch(env, batch_size, t_max=500)\n",
    "    \n",
    "    \n",
    "    # train on the states using actions as targets\n",
    "    for s_i in range(len(states)):\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        tensor_states = torch.FloatTensor(states[s_i])\n",
    "        tensor_actions = torch.LongTensor(actions[s_i])\n",
    "        action_scores_v = net(tensor_states)\n",
    "        loss_v = objective(action_scores_v, tensor_actions)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #show results\n",
    "    mean_reward = np.mean(rewards)\n",
    "    threshold = 0.1\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, threshold=%.1f\" % (\n",
    "            i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from moviepy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from moviepy) (1.24.3)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from moviepy) (2.33.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (69.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n",
      "Requirement already satisfied: ffmpeg in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install moviepy\n",
    "!pip install ffmpeg --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, \"video\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "total_reward = 0.0\n",
    "done = False\n",
    "action\n",
    "tensor_states = torch.FloatTensor([state])\n",
    "action_scores_v = net(tensor_states)\n",
    "while not done:\n",
    "        tensor_states = torch.FloatTensor([state])\n",
    "        action_scores_v = net(tensor_states)\n",
    "\n",
    "         act_probs_v = activation(net(s_v))\n",
    "        new_s, r, done, _, _ = env.step(a)\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        action_values = dqn_agent.q_network(state)\n",
    "        action = np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "6_LunarLander_PolicyBased.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
