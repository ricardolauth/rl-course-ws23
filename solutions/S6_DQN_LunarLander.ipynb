{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCLrRFHSKl_5"
   },
   "source": [
    "# Deep Q-Network with Lunar Lander\n",
    "\n",
    "This notebook shows an implementation of a DQN on the LunarLander environment.\n",
    "Details on the environment can be found [here](https://gym.openai.com/envs/LunarLander-v2/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2RNqaAGiLU1"
   },
   "source": [
    "## 1. Setup\n",
    "\n",
    "We first need to install some dependencies for using the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96dExX1TKm2m",
    "outputId": "4a3a6d38-7cae-4c85-dc5f-c0182c2e8275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (0.41.2)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.42.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (68.0.0)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-69.0.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pip in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (23.3.1)\n",
      "Using cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "Using cached setuptools-69.0.2-py3-none-any.whl (819 kB)\n",
      "Installing collected packages: wheel, setuptools\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.41.2\n",
      "    Uninstalling wheel-0.41.2:\n",
      "      Successfully uninstalled wheel-0.41.2\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 68.0.0\n",
      "    Uninstalling setuptools-68.0.0:\n",
      "      Successfully uninstalled setuptools-68.0.0\n",
      "Successfully installed setuptools-69.0.2 wheel-0.42.0\n",
      "Requirement already satisfied: swig in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (4.1.1.post1)\n",
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from gymnasium[box2d]) (1.24.3)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from gymnasium[box2d]) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from gymnasium[box2d]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from gymnasium[box2d]) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from gymnasium[box2d]) (6.0.0)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pygame==2.1.3 (from gymnasium[box2d])\n",
      "  Using cached pygame-2.1.3-cp38-cp38-win_amd64.whl (10.4 MB)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from gymnasium[box2d]) (4.1.1.post1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.11.0)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py): started\n",
      "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for box2d-py\n",
      "Failed to build box2d-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [28 lines of output]\n",
      "      Using setuptools (version 69.0.2).\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-38\n",
      "      creating build\\lib.win-amd64-cpython-38\\Box2D\n",
      "      copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-cpython-38\\Box2D\n",
      "      copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-cpython-38\\Box2D\n",
      "      creating build\\lib.win-amd64-cpython-38\\Box2D\\b2\n",
      "      copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-cpython-38\\Box2D\\b2\n",
      "      running build_ext\n",
      "      building 'Box2D._Box2D' extension\n",
      "      swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "      swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "      Box2D\\Common\\b2Math.h(67) : Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(47) : Warning 302: %extend definition of 'b2Vec2'.\n",
      "      Box2D\\Common\\b2Math.h(158) : Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(168) : Warning 302: %extend definition of 'b2Vec3'.\n",
      "      Box2D\\Common\\b2Math.h(197) : Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(301) : Warning 302: %extend definition of 'b2Mat22'.\n",
      "      Box2D\\Common\\b2Math.h(271) : Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(372) : Warning 302: %extend definition of 'b2Mat33'.\n",
      "      Box2D\\Collision\\b2DynamicTree.h(44) : Warning 312: Nested union not currently supported (ignored).\n",
      "      Box2D\\Common\\b2Settings.h(144) : Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "      Box2D\\Common\\b2Math.h(91) : Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "      Box2D\\Common\\b2Math.h(85) : Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for box2d-py\n",
      "ERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wheel setuptools pip --upgrade\n",
    "!pip3 install \"swig\"\n",
    "!pip3 install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CZXskDwXKl_-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from time import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVO0INWR1DYS",
    "outputId": "68e453b9-79a7-4921-ac7c-186de15ec6c5"
   },
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Box2D is not installed, run `pip install gymnasium[box2d]`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:15\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLunarLander-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages\\gymnasium\\envs\\registration.py:755\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    752\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[0;32m    758\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages\\gymnasium\\envs\\registration.py:553\u001b[0m, in \u001b[0;36mload_env_creator\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    552\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 553\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[1;32mc:\\Users\\ricar\\miniconda3\\envs\\rl-course\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:961\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages\\gymnasium\\envs\\box2d\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbipedal_walker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcar_racing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CarRacing\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlunar_lander\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[1;32mc:\\Users\\ricar\\miniconda3\\envs\\rl-course\\lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:25\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox2D is not installed, run `pip install gymnasium[box2d]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: Box2D is not installed, run `pip install gymnasium[box2d]`"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lrq9VwzO1Zx4",
    "outputId": "ae7034e1-9628-4794-bba6-f7dac7ad5de4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKg3BvSnivPE"
   },
   "source": [
    "## 2. Define the neural network, the replay buffer and the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9pG_Ii7jToR"
   },
   "source": [
    "First, we define the neural network that predicts the Q-values for all actions, given a state as input.\n",
    "This is a fully-connected neural net with two hidden layers using Relu activations.\n",
    "The last layer does not have any activation and outputs a Q-value for every action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "JFxqeLkf1eHY"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0xHaPTIj1pD"
   },
   "source": [
    "Next, we define a replay buffer that saves previous transitions and provides a `sample` function to randomly extract a batch of experiences from the buffer.\n",
    "\n",
    "Note that experiences are internally saved as `numpy`-arrays. They are converted back to PyTorch tensors before being returned by the `sample`-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gQw6QVAC1hQf"
   },
   "outputs": [],
   "source": [
    "class StateTransition:\n",
    "    def __init__(self, state, action, reward, next_state, done):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.done = 1 if done else 0 # Convert done flag from boolean to int\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "       \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        state_transition = StateTransition(state, action, reward, next_state, done)\n",
    "        self.memory.append(state_transition)\n",
    "                \n",
    "    def sample(self):\n",
    "        state_transitions = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        states = np.vstack([s_t.state for s_t in state_transitions])\n",
    "        states_tensor = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "        actions = np.vstack([s_t.action for s_t in state_transitions])\n",
    "        actions_tensor = torch.from_numpy(actions).long().to(device)\n",
    "\n",
    "        rewards = np.vstack([s_t.reward for s_t in state_transitions])\n",
    "        rewards_tensor = torch.from_numpy(rewards).float().to(device)\n",
    "\n",
    "        next_states = np.vstack([s_t.next_state for s_t in state_transitions])\n",
    "        next_states_tensor = torch.from_numpy(next_states).float().to(device)\n",
    "        \n",
    "        dones = np.vstack([s_t.done for s_t in state_transitions])\n",
    "        dones_tensor = torch.from_numpy(dones).float().to(device)\n",
    "        \n",
    "        return (states_tensor, actions_tensor, rewards_tensor, next_states_tensor, dones_tensor)\n",
    "        \n",
    "    def is_filled(self):\n",
    "        return len(self.memory) >= BATCH_SIZE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "IYjlS7Fy1jJA"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000    # Replay memory size\n",
    "BATCH_SIZE = 64         # Number of experiences to sample from memory\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TARGET_SYNC = 20        # How often the target networks is synchronized\n",
    "       \n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Initialize Q and Target Q networks\n",
    "        self.q_network = QNetwork(state_size, action_size).to(device)\n",
    "        self.target_network = QNetwork(state_size, action_size).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        \n",
    "        # Initiliase replay buffer \n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        self.timestep = 0\n",
    "    \n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.timestep += 1\n",
    "        \n",
    "        if not self.memory.is_filled(): # train only when buffer is filled\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
    "               \n",
    "        # you need to implement the following method in task 5\n",
    "        loss = self.calculate_loss(states, actions, rewards, next_states, dones) \n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Synchronize target network by copying weights\n",
    "        if self.timestep % TARGET_SYNC == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    \n",
    "    def calculate_loss(self, states, actions, rewards, next_states, dones):\n",
    "    \n",
    "        action_values = self.target_network(next_states).detach()\n",
    "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # If \"done==1\" just use reward, else update Q_target with discounted action values\n",
    "        Q_target = rewards + (GAMMA * max_action_values * (1 - dones))\n",
    "        Q_prediction = self.q_network(states).gather(1, actions)\n",
    "\n",
    "        # Calculate loss and update weights\n",
    "        loss = F.mse_loss(Q_prediction, Q_target)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def choose_action(self, state, epsilon):\n",
    "        rnd = random.random()\n",
    "        if rnd < epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            action_values = self.q_network(state)\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2P3-UIm0fh3W"
   },
   "source": [
    "### 3. Executes episodes and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NikAZhjNfsoi"
   },
   "source": [
    "We first define the necessary paramters for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "lJGrZry81pu4"
   },
   "outputs": [],
   "source": [
    "TARGET_SCORE = 200            # Train until this score is reached\n",
    "MAX_EPISODE_LENGTH = 1000     # Max steps allowed in a single episode\n",
    "EPSILON_MIN = 0.01            # Minimum epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezOn9IpKf17C"
   },
   "source": [
    "Then we start executing episodes and observe the mean score per episode.\n",
    "The environment is considered as solved if this score is above 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_EC7XLJ1slY",
    "outputId": "ad110ade-36e5-4600-fcf0-faf2edc07235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 8, action size: 4\n",
      "After 10 episodes, average score is -260.13. Took 2 seconds.\n",
      "After 20 episodes, average score is -219.62. Took 7 seconds.\n",
      "After 30 episodes, average score is -185.18. Took 15 seconds.\n",
      "After 40 episodes, average score is -186.60. Took 12 seconds.\n",
      "After 50 episodes, average score is -174.94. Took 10 seconds.\n",
      "After 60 episodes, average score is -160.57. Took 11 seconds.\n",
      "After 70 episodes, average score is -152.14. Took 10 seconds.\n",
      "After 80 episodes, average score is -141.16. Took 15 seconds.\n",
      "After 90 episodes, average score is -133.24. Took 15 seconds.\n",
      "After 100 episodes, average score is -131.53. Took 14 seconds.\n",
      "After 110 episodes, average score is -113.75. Took 14 seconds.\n",
      "After 120 episodes, average score is -101.41. Took 17 seconds.\n",
      "After 130 episodes, average score is -94.43. Took 14 seconds.\n",
      "After 140 episodes, average score is -86.77. Took 16 seconds.\n",
      "After 150 episodes, average score is -78.77. Took 18 seconds.\n",
      "After 160 episodes, average score is -75.36. Took 17 seconds.\n",
      "After 170 episodes, average score is -69.12. Took 16 seconds.\n",
      "After 180 episodes, average score is -66.43. Took 19 seconds.\n",
      "After 190 episodes, average score is -63.87. Took 18 seconds.\n",
      "After 200 episodes, average score is -55.19. Took 12 seconds.\n",
      "After 210 episodes, average score is -42.60. Took 15 seconds.\n",
      "After 220 episodes, average score is -35.72. Took 15 seconds.\n",
      "After 230 episodes, average score is -20.78. Took 16 seconds.\n",
      "After 240 episodes, average score is 4.00. Took 12 seconds.\n",
      "After 250 episodes, average score is 23.62. Took 14 seconds.\n",
      "After 260 episodes, average score is 45.13. Took 12 seconds.\n",
      "After 270 episodes, average score is 70.59. Took 9 seconds.\n",
      "After 280 episodes, average score is 93.16. Took 9 seconds.\n",
      "After 290 episodes, average score is 114.82. Took 8 seconds.\n",
      "After 300 episodes, average score is 136.98. Took 11 seconds.\n",
      "After 310 episodes, average score is 147.99. Took 9 seconds.\n",
      "After 320 episodes, average score is 149.78. Took 9 seconds.\n",
      "After 330 episodes, average score is 158.92. Took 13 seconds.\n",
      "After 340 episodes, average score is 160.13. Took 9 seconds.\n",
      "After 350 episodes, average score is 159.14. Took 9 seconds.\n",
      "After 360 episodes, average score is 161.92. Took 11 seconds.\n",
      "After 370 episodes, average score is 163.84. Took 9 seconds.\n",
      "After 380 episodes, average score is 167.66. Took 9 seconds.\n",
      "After 390 episodes, average score is 170.42. Took 11 seconds.\n",
      "After 400 episodes, average score is 172.62. Took 9 seconds.\n",
      "After 410 episodes, average score is 177.14. Took 8 seconds.\n",
      "After 420 episodes, average score is 193.84. Took 8 seconds.\n",
      "After 430 episodes, average score is 197.45. Took 7 seconds.\n",
      "Environment solved in 432 episodes. Average score: 201.82\n",
      "Took 515 seconds (~8.0 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Get state and action sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(f'State size: {state_size}, action size: {action_size}')\n",
    "dqn_agent = DQNAgent(state_size, action_size)\n",
    "start = time()\n",
    "last_time = start\n",
    "\n",
    "scores_window = deque(maxlen=100)\n",
    "mean_score = 0\n",
    "episode = 0\n",
    "\n",
    "while True:\n",
    "    episode += 1\n",
    "    score = 0\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    for t in range(MAX_EPISODE_LENGTH):\n",
    "        \n",
    "        epsilon = max(1/episode, EPSILON_MIN)\n",
    "        action = dqn_agent.choose_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        dqn_agent.train(state, action, reward, next_state, done)\n",
    "        state = next_state        \n",
    "        score += reward        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scores_window.append(score)\n",
    "    mean_score = np.mean(scores_window)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f'After {episode} episodes, average score is {mean_score:.2f}. ', end='')\n",
    "        print(f'Took {time()-last_time:.0f} seconds.')\n",
    "        last_time = time()\n",
    "    \n",
    "    if mean_score >= TARGET_SCORE:\n",
    "        print(f'Environment solved in {episode} episodes. Average score: {mean_score:.2f}')\n",
    "        break\n",
    "\n",
    "print(f'Took {time()-start:.0f} seconds (~{(time()-start)//60} minutes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks: Implement the following functions to make the code above work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the agent train something, we need to implement the `calculate_loss` function in the code above. To make this easier, we do this along the following mini tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, were are given a tiny replay buffer that contains only two transitions of the form `state`, `action`, `reward`, `next_state` and `done`. \n",
    "\n",
    "The resulting tensors `states`, `actions`, `rewards`, `next_states` and `dones` are of the same format as the input to the function `calculate_loss`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = [ 0.64,  0.38,  0.04, -0.10, -0.22, -.00,  0.00,  0.00]\n",
    "state_2 = [ 0.00,  0.35,  0.41, -0.59, -0.66, -0.23,  0.00,  0.00]\n",
    "states = torch.FloatTensor([state_1, state_2])\n",
    "\n",
    "actions = torch.LongTensor([[2],[1]])\n",
    "\n",
    "rewards = torch.FloatTensor([[1.8670],[1.2630]])\n",
    "\n",
    "next_state_1 = [-0.60,  0.94, -0.04, -0.13,  0.27, 0.70,  0.00,  0.00]\n",
    "next_state_2 = [-0.60,  0.94, -0.04, -0.13,  0.27, 0.70,  0.00,  0.00]\n",
    "next_states = torch.FloatTensor([next_state_1, next_state_2])\n",
    "\n",
    "dones = torch.FloatTensor([[0],[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first calculate the Q-Learning target. In a first step we use the `target_network` to calculate the Q-values for every state in the `next_states` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[38.2709, 32.6704, 39.4406, 41.4000],\n",
       "        [38.2709, 32.6704, 39.4406, 41.4000]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = dqn_agent.target_network(next_states)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not want to backpropagate on these values, we detach them from the computational graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[38.2709, 32.6704, 39.4406, 41.4000],\n",
       "        [38.2709, 32.6704, 39.4406, 41.4000]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = q_values.detach()\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using Q-Learning, we are only interested in the maximum value per line.\n",
    "Implement some code that squashed the above to a torch tensor of shape `[2, 1]` that contains for every state only the maximum Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[41.4000],\n",
       "        [41.4000]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_q_values = q_values.max(1)[0].unsqueeze(1)\n",
    "max_q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to calcualte the Q-Learning targets using the tensors `rewards` and `dones` as seen in the lecture. Remember: The target consist only of the reward if the done flag is set for a transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42.8530],\n",
       "        [ 1.2630]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "Q_targets = rewards + (GAMMA * max_q_values * (1 - dones))\n",
    "Q_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now caluclate the predicton of the network on the current states. For this we use the `q_network` of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32.8599, 36.9284, 34.7862, 32.6744],\n",
       "        [68.1535, 81.2231, 71.1459, 72.2256]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = dqn_agent.q_network(states)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns for every state the Q-values for all actions. However, we only need the q-values of the according  that was actually taken in this transition (this is stored in `actions`).\n",
    "Next, extract the Q-Value for the taken action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[34.7862],\n",
       "        [81.2231]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_value_action = predictions.gather(1, actions)\n",
    "q_value_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values can now be used to define the loss for the current batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.mse_loss(q_value_action, Q_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 3:\n",
    "Use the code from these examples to implement the `calculate_loss` function from above and train the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Subtask 4:\n",
    "Use the trained model to play and record one episode. The recorded video will be stored into the video-subfolder on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (1.24.3)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (2.33.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from imageio<3.0,>=2.5->moviepy) (10.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (68.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2023.11.17)\n",
      "Requirement already satisfied: ffmpeg in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install moviepy\n",
    "!pip install ffmpeg --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/pbaier/code/rl-course-ws23/solutions/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/pbaier/code/rl-course-ws23/solutions/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/pbaier/code/rl-course-ws23/solutions/video/rl-video-episode-0.mp4\n",
      "Total reward: 251.9432109625641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, \"video\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "total_reward = 0.0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        action_values = dqn_agent.q_network(state)\n",
    "        action = np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "5_DQN_LunarLander.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
