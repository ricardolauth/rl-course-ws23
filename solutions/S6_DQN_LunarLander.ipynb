{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCLrRFHSKl_5"
   },
   "source": [
    "# Deep Q-Network with Lunar Lander\n",
    "\n",
    "This notebook shows an implementation of a DQN on the LunarLander environment.\n",
    "Details on the environment can be found [here](https://gym.openai.com/envs/LunarLander-v2/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2RNqaAGiLU1"
   },
   "source": [
    "## 1. Setup\n",
    "\n",
    "We first need to install some dependencies for using the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96dExX1TKm2m",
    "outputId": "4a3a6d38-7cae-4c85-dc5f-c0182c2e8275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (1.24.3)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (6.0.0)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame==2.1.3 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (2.1.3)\n",
      "Requirement already satisfied: swig==4.* in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from gymnasium[box2d]) (4.1.1.post0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install 'gymnasium[box2d]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CZXskDwXKl_-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from time import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVO0INWR1DYS",
    "outputId": "68e453b9-79a7-4921-ac7c-186de15ec6c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbec3b1f4f0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lrq9VwzO1Zx4",
    "outputId": "ae7034e1-9628-4794-bba6-f7dac7ad5de4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKg3BvSnivPE"
   },
   "source": [
    "## 2. Define the neural network, the replay buffer and the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9pG_Ii7jToR"
   },
   "source": [
    "First, we define the neural network that predicts the Q-values for all actions, given a state as input.\n",
    "This is a fully-connected neural net with two hidden layers using Relu activations.\n",
    "The last layer does not have any activation and outputs a Q-value for every action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "JFxqeLkf1eHY"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0xHaPTIj1pD"
   },
   "source": [
    "Next, we define a replay buffer that saves previous transitions and provides a `sample` function to randomly extract a batch of experiences from the buffer.\n",
    "\n",
    "Note that experiences are internally saved as `numpy`-arrays. They are converted back to PyTorch tensors before being returned by the `sample`-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gQw6QVAC1hQf"
   },
   "outputs": [],
   "source": [
    "class StateTransition:\n",
    "    def __init__(self, state, action, reward, next_state, done):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.done = 1 if done else 0 # Convert done flag from boolean to int\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "       \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        state_transition = StateTransition(state, action, reward, next_state, done)\n",
    "        self.memory.append(state_transition)\n",
    "                \n",
    "    def sample(self):\n",
    "        state_transitions = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        states = np.vstack([s_t.state for s_t in state_transitions])\n",
    "        states_tensor = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "        actions = np.vstack([s_t.action for s_t in state_transitions])\n",
    "        actions_tensor = torch.from_numpy(actions).long().to(device)\n",
    "\n",
    "        rewards = np.vstack([s_t.reward for s_t in state_transitions])\n",
    "        rewards_tensor = torch.from_numpy(rewards).float().to(device)\n",
    "\n",
    "        next_states = np.vstack([s_t.next_state for s_t in state_transitions])\n",
    "        next_states_tensor = torch.from_numpy(next_states).float().to(device)\n",
    "        \n",
    "        dones = np.vstack([s_t.done for s_t in state_transitions])\n",
    "        dones_tensor = torch.from_numpy(dones).float().to(device)\n",
    "        \n",
    "        return (states_tensor, actions_tensor, rewards_tensor, next_states_tensor, dones_tensor)\n",
    "        \n",
    "    def is_filled(self):\n",
    "        return len(self.memory) >= BATCH_SIZE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "IYjlS7Fy1jJA"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000    # Replay memory size\n",
    "BATCH_SIZE = 64         # Number of experiences to sample from memory\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TARGET_SYNC = 20        # How often the target networks is synchronized\n",
    "       \n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Initialize Q and Target Q networks\n",
    "        self.q_network = QNetwork(state_size, action_size).to(device)\n",
    "        self.target_network = QNetwork(state_size, action_size).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        \n",
    "        # Initiliase replay buffer \n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        self.timestep = 0\n",
    "    \n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.timestep += 1\n",
    "        \n",
    "        if not self.memory.is_filled(): # train only when buffer is filled\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
    "               \n",
    "        # you need to implement the following method in task 5\n",
    "        loss = self.calculate_loss(states, actions, rewards, next_states, dones) \n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Synchronize target network by copying weights\n",
    "        if self.timestep % TARGET_SYNC == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    \n",
    "    def calculate_loss(self, states, actions, rewards, next_states, dones):\n",
    "    \n",
    "        action_values = self.target_network(next_states).detach()\n",
    "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # If \"done==1\" just use reward, else update Q_target with discounted action values\n",
    "        Q_target = rewards + (GAMMA * max_action_values * (1 - dones))\n",
    "        Q_prediction = self.q_network(states).gather(1, actions)\n",
    "\n",
    "        # Calculate loss and update weights\n",
    "        loss = F.mse_loss(Q_prediction, Q_target)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def choose_action(self, state, epsilon):\n",
    "        rnd = random.random()\n",
    "        if rnd < epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            action_values = self.q_network(state)\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2P3-UIm0fh3W"
   },
   "source": [
    "### 3. Executes episodes and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NikAZhjNfsoi"
   },
   "source": [
    "We first define the necessary paramters for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "lJGrZry81pu4"
   },
   "outputs": [],
   "source": [
    "TARGET_SCORE = 200            # Train until this score is reached\n",
    "MAX_EPISODE_LENGTH = 1000     # Max steps allowed in a single episode\n",
    "EPSILON_MIN = 0.01            # Minimum epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezOn9IpKf17C"
   },
   "source": [
    "Then we start executing episodes and observe the mean score per episode.\n",
    "The environment is considered as solved if this score is above 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_EC7XLJ1slY",
    "outputId": "ad110ade-36e5-4600-fcf0-faf2edc07235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 8, action size: 4\n",
      "After 10 episodes, average score is -260.13. Took 2 seconds.\n",
      "After 20 episodes, average score is -219.62. Took 7 seconds.\n",
      "After 30 episodes, average score is -185.18. Took 15 seconds.\n",
      "After 40 episodes, average score is -186.60. Took 12 seconds.\n",
      "After 50 episodes, average score is -174.94. Took 10 seconds.\n",
      "After 60 episodes, average score is -160.57. Took 11 seconds.\n",
      "After 70 episodes, average score is -152.14. Took 10 seconds.\n",
      "After 80 episodes, average score is -141.16. Took 15 seconds.\n",
      "After 90 episodes, average score is -133.24. Took 15 seconds.\n",
      "After 100 episodes, average score is -131.53. Took 14 seconds.\n",
      "After 110 episodes, average score is -113.75. Took 14 seconds.\n",
      "After 120 episodes, average score is -101.41. Took 17 seconds.\n",
      "After 130 episodes, average score is -94.43. Took 14 seconds.\n",
      "After 140 episodes, average score is -86.77. Took 16 seconds.\n",
      "After 150 episodes, average score is -78.77. Took 18 seconds.\n",
      "After 160 episodes, average score is -75.36. Took 17 seconds.\n",
      "After 170 episodes, average score is -69.12. Took 16 seconds.\n",
      "After 180 episodes, average score is -66.43. Took 19 seconds.\n",
      "After 190 episodes, average score is -63.87. Took 18 seconds.\n",
      "After 200 episodes, average score is -55.19. Took 12 seconds.\n",
      "After 210 episodes, average score is -42.60. Took 15 seconds.\n",
      "After 220 episodes, average score is -35.72. Took 15 seconds.\n",
      "After 230 episodes, average score is -20.78. Took 16 seconds.\n",
      "After 240 episodes, average score is 4.00. Took 12 seconds.\n",
      "After 250 episodes, average score is 23.62. Took 14 seconds.\n",
      "After 260 episodes, average score is 45.13. Took 12 seconds.\n",
      "After 270 episodes, average score is 70.59. Took 9 seconds.\n",
      "After 280 episodes, average score is 93.16. Took 9 seconds.\n",
      "After 290 episodes, average score is 114.82. Took 8 seconds.\n",
      "After 300 episodes, average score is 136.98. Took 11 seconds.\n",
      "After 310 episodes, average score is 147.99. Took 9 seconds.\n",
      "After 320 episodes, average score is 149.78. Took 9 seconds.\n",
      "After 330 episodes, average score is 158.92. Took 13 seconds.\n",
      "After 340 episodes, average score is 160.13. Took 9 seconds.\n",
      "After 350 episodes, average score is 159.14. Took 9 seconds.\n",
      "After 360 episodes, average score is 161.92. Took 11 seconds.\n",
      "After 370 episodes, average score is 163.84. Took 9 seconds.\n",
      "After 380 episodes, average score is 167.66. Took 9 seconds.\n",
      "After 390 episodes, average score is 170.42. Took 11 seconds.\n",
      "After 400 episodes, average score is 172.62. Took 9 seconds.\n",
      "After 410 episodes, average score is 177.14. Took 8 seconds.\n",
      "After 420 episodes, average score is 193.84. Took 8 seconds.\n",
      "After 430 episodes, average score is 197.45. Took 7 seconds.\n",
      "Environment solved in 432 episodes. Average score: 201.82\n",
      "Took 515 seconds (~8.0 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Get state and action sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(f'State size: {state_size}, action size: {action_size}')\n",
    "dqn_agent = DQNAgent(state_size, action_size)\n",
    "start = time()\n",
    "last_time = start\n",
    "\n",
    "scores_window = deque(maxlen=100)\n",
    "mean_score = 0\n",
    "episode = 0\n",
    "\n",
    "while True:\n",
    "    episode += 1\n",
    "    score = 0\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    for t in range(MAX_EPISODE_LENGTH):\n",
    "        \n",
    "        epsilon = max(1/episode, EPSILON_MIN)\n",
    "        action = dqn_agent.choose_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        dqn_agent.train(state, action, reward, next_state, done)\n",
    "        state = next_state        \n",
    "        score += reward        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scores_window.append(score)\n",
    "    mean_score = np.mean(scores_window)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f'After {episode} episodes, average score is {mean_score:.2f}. ', end='')\n",
    "        print(f'Took {time()-last_time:.0f} seconds.')\n",
    "        last_time = time()\n",
    "    \n",
    "    if mean_score >= TARGET_SCORE:\n",
    "        print(f'Environment solved in {episode} episodes. Average score: {mean_score:.2f}')\n",
    "        break\n",
    "\n",
    "print(f'Took {time()-start:.0f} seconds (~{(time()-start)//60} minutes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks: Implement the following functions to make the code above work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the agent train something, we need to implement the `calculate_loss` function in the code above. To make this easier, we do this along the following mini tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, were are given a tiny replay buffer that contains only two transitions of the form `state`, `action`, `reward`, `next_state` and `done`. \n",
    "\n",
    "The resulting tensors `states`, `actions`, `rewards`, `next_states` and `dones` are of the same format as the input to the function `calculate_loss`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = [ 0.64,  0.38,  0.04, -0.10, -0.22, -.00,  0.00,  0.00]\n",
    "state_2 = [ 0.00,  0.35,  0.41, -0.59, -0.66, -0.23,  0.00,  0.00]\n",
    "states = torch.FloatTensor([state_1, state_2])\n",
    "\n",
    "actions = torch.LongTensor([[2],[1]])\n",
    "\n",
    "rewards = torch.FloatTensor([[1.8670],[1.2630]])\n",
    "\n",
    "next_state_1 = [-0.60,  0.94, -0.04, -0.13,  0.27, 0.70,  0.00,  0.00]\n",
    "next_state_2 = [-0.60,  0.94, -0.04, -0.13,  0.27, 0.70,  0.00,  0.00]\n",
    "next_states = torch.FloatTensor([next_state_1, next_state_2])\n",
    "\n",
    "dones = torch.FloatTensor([[0],[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first calculate the Q-Learning target. In a first step we use the `target_network` to calculate the Q-values for every state in the `next_states` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[38.2709, 32.6704, 39.4406, 41.4000],\n",
       "        [38.2709, 32.6704, 39.4406, 41.4000]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = dqn_agent.target_network(next_states)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not want to backpropagate on these values, we detach them from the computational graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[38.2709, 32.6704, 39.4406, 41.4000],\n",
       "        [38.2709, 32.6704, 39.4406, 41.4000]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = q_values.detach()\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using Q-Learning, we are only interested in the maximum value per line.\n",
    "Implement some code that squashed the above to a torch tensor of shape `[2, 1]` that contains for every state only the maximum Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[41.4000],\n",
       "        [41.4000]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_q_values = q_values.max(1)[0].unsqueeze(1)\n",
    "max_q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to calcualte the Q-Learning targets using the tensors `rewards` and `dones` as seen in the lecture. Remember: The target consist only of the reward if the done flag is set for a transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42.8530],\n",
       "        [ 1.2630]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "Q_targets = rewards + (GAMMA * max_q_values * (1 - dones))\n",
    "Q_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now caluclate the predicton of the network on the current states. For this we use the `q_network` of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32.8599, 36.9284, 34.7862, 32.6744],\n",
       "        [68.1535, 81.2231, 71.1459, 72.2256]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = dqn_agent.q_network(states)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns for every state the Q-values for all actions. However, we only need the q-values of the according  that was actually taken in this transition (this is stored in `actions`).\n",
    "Next, extract the Q-Value for the taken action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[34.7862],\n",
       "        [81.2231]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_value_action = predictions.gather(1, actions)\n",
    "q_value_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values can now be used to define the loss for the current batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.mse_loss(q_value_action, Q_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtask 3:\n",
    "Use the code from these examples to implement the `calculate_loss` function from above and train the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Subtask 4:\n",
    "Use the trained model to play and record one episode. The recorded video will be stored into the video-subfolder on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (1.24.3)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (2.33.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from imageio<3.0,>=2.5->moviepy) (10.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (68.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2023.11.17)\n",
      "Requirement already satisfied: ffmpeg in /Users/pbaier/miniconda3/envs/rl-test3/lib/python3.8/site-packages (1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install moviepy\n",
    "!pip install ffmpeg --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/pbaier/code/rl-course-ws23/solutions/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/pbaier/code/rl-course-ws23/solutions/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/pbaier/code/rl-course-ws23/solutions/video/rl-video-episode-0.mp4\n",
      "Total reward: 251.9432109625641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, \"video\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "total_reward = 0.0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        action_values = dqn_agent.q_network(state)\n",
    "        action = np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "5_DQN_LunarLander.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
